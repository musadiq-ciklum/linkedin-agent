# ğŸ¤– AI-Agentic RAG Assistant

This project is the final assignment for the **Ciklum AI Academy â€“ Engineering Track**.  
It demonstrates an **AI-Agentic system** built on a **Retrieval-Augmented Generation (RAG)** pipeline with autonomous reasoning, tool-calling, and self-reflection.

---

## ğŸ“˜ Project Overview

The AI-Agentic RAG Assistant:

- ğŸ” Uses a RAG pipeline for information retrieval from prepared datasets
- ğŸ§  Performs autonomous reasoning and self-reflection
- ğŸ› ï¸ Executes tool-based actions based on reasoning outcomes
- ğŸ“Š Measures effectiveness via relevance, clarity, and accuracy metrics
- âš™ï¸ Supports flexible data and retrieval configuration via `.env` and `config.py`

---

## ğŸš€ Setup

### ğŸ“¥ Clone the Repository

**SSH**
```bash
git clone git@github.com:musadiq-ciklum/linkedin-agent.git
```

**HTTPS**
```bash
git clone https://github.com/musadiq-ciklum/linkedin-agent.git
```

##  ğŸ§ª Create a Virtual Environment & Install Dependencies
```bash
python -m venv venv
source venv/bin/activate      # Linux/macOS
venv\Scripts\activate         # Windows
pip install -r requirements.txt
```


## ğŸ” Configure Environment Variables
Create a copy of `.env.example` and rename it to `.env`.

**Set your Gemini key**
```
GEMINI_API_KEY=your_real_key_here
```

## âœ… Verify Setup
```
pytest -v
```

## ğŸ§­ Usage

**ğŸ“š Data Preparation**
```
python scripts/data_prep.py data/raw/sample.txt
```
Prepares documents and embeddings for the RAG pipeline.

**ğŸ—‚ï¸ Populate Vector Store (Chroma)**
```
python scripts/populate_chroma_test.py
```
Loads embeddings into a local Chroma vector database.

**ğŸ” Query / Search**
```
python scripts/search_test.py "Search query"
```
Performs semantic search over the vector store and returns top-k relevant documents based on configured thresholds.

**ğŸ¤– Run Agent**
```
python scripts/rag/rag_run.py "Search query"
```
Demonstrates the AI-Agentic workflow, including retrieval, reasoning, tool-calling, self-reflection, and final response generation.

## ğŸŒ API Usage (FastAPI + Uvicorn)
The project exposes HTTP endpoints for querying the agent, generating embeddings, and ingesting new documents into the vector store.

**â–¶ï¸ Start API Server**
```bash
uvicorn src.api.main:app --reload
```
Once running, the API will be available at:
```bash
http://127.0.0.1:8000/docs
```
Interactive API documentation is available via Swagger UI.

**POST /ask**
- Runs the full agentic RAG pipeline:
- Retrieves relevant documents from the vector store
- Performs LLM-based reasoning and optional re-ranking
- Generates a final, context-aware response

**POST /embedding**
Generates a vector embedding for the provided input text using the configured embedding model.

**POST /upload**
Uploads a .txt or .pdf document and ingests it into the vector store:

- Extracts and cleans text
- Chunks content
- Generates embeddings
- Stores vectors for future retrieval

## ğŸ§° Technology Stack

This project is built using the following technologies and libraries:

- **Python 3.10+** â€“ Core programming language
- **FastAPI** â€“ API framework for exposing agent endpoints
- **Uvicorn** â€“ ASGI server for running the FastAPI application
- **Google Gemini API** â€“ Large Language Model used for reasoning and response generation
- **Sentence-Transformers (MiniLM)** â€“ Embedding generation for semantic retrieval
- **ChromaDB** â€“ Vector database for storing and retrieving embeddings
- **Pytest** â€“ Unit testing and validation
- **dotenv** â€“ Environment variable management
- **Mermaid (architecture.mmd)** â€“ High-level system architecture visualization

The system follows a modular, agentic RAG design with configurable retrieval, reasoning, tool execution, and self-reflection components.

## ğŸ“ Evaluation

The system includes an offline evaluation pipeline to measure retrieval quality, answer relevance, and performance characteristics of the RAG workflow.

Evaluation is performed using a small labeled dataset and compares agent behavior **with and without reranking**.

**Metrics used:**
- **Precision@K** â€“ Measures how many retrieved documents are relevant
- **Recall@K** â€“ Measures coverage of relevant documents
- **RAG Quality Score** â€“ Keyword overlap between generated and expected answers
- **Latency Metrics** â€“ Retrieval, reranking, LLM, and total response time

**Run Evaluation**
```bash
python evaluation/eval_rag.py
```

This script:
- Executes the RAG pipeline on a sample evaluation dataset
- Compares reranked vs non-reranked retrieval
- Exports CSV reports to:
  - evaluation/reports/report_with_rerank.csv
  - evaluation/reports/report_without_rerank.csv

The evaluation setup ensures the agentâ€™s retrieval effectiveness, reasoning quality, and performance characteristics can be inspected and compared in a reproducible manner.


## âš™ï¸ Configuration
All parameters are centralized in `src/config.py`. 

Key options:

- `GEMINI_MODEL_NAME` â€“ LLM model name (Gemini 2.5 Flash)
- `EMBEDDING_MODEL_NAME` â€“ Embedding model
- `CHROMA_DIR` â€“ Local vector store directory
- `MIN_RELEVANCE_SCORE` â€“ Threshold for document relevance
- `EXTRACTIVE_SCORE_THRESHOLD` â€“ Threshold for extractive answers
- `DEFAULT_TOP_K` â€“ Number of documents retrieved per query
- `MAX_CONTEXT_DOCS` â€“ Maximum number of context documents for reasoning